h1. Document Clustering

Here we use K-means clustering to classify a set of raw text documents.

h3. TFIDF

First we need to run tf-idf over our documents to vectorize them. It is assumed that your documents are tab-separated where the first field is the document id and the second field is the document text that contains *no* newlines.

<pre><code>
pig -p DOCS=/path/to/my_docs -p NDOCS=<num_docs> -p TFIDF=/path/to/output tfidf.pig
</code></pre>

h3. K Centers

This is the tricky step. There's a pig sampler that can uniformly sample your data and generate K initial centers but it's hacky. To use it do:

<pre><code>
pig -p TFIDF=/path/to/tfidf-output -p CENTERS=/path/to/output sample_k_centers.pig
</code></pre>

You can also generate your own centers as long as they can be read into the following pig schema:

<pre><code>
(doc_id:chararray, vector:bag {t:tuple (token:chararray, weight:double)})
</code></pre>

h3. Iteration

To run a single iteration that clusters document vectors around the K centers, and computes centroids, do:

<pre><code>
pig -p TFIDF=/path/to/tfidf-output -p MAX_CENTER_SIZE=<max_center_size> -p CURR_CENTERS=/path/to/current-centers -p NEXT_CENTERS=/path/to/next-centers cluster_documents.pig
</code></pre>

where <max_center_size> is the maximum allowable center vector length.

There's a shell script that can run it multiple times called clusterer.sh:

<pre><code>
./clusterer.sh /path/to/workdir <max_center_size> <num_iterations> <start_iteration>
</code></pre>

which assumes you've already ran tfidf and sampled k centers and placed their outputs in the workdir named 'tfidf-vectors' and 'k_centers-0' resplectively.
